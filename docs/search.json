[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts.\nTesting for Michael"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Fixing factory machines faster with LLMs",
    "section": "",
    "text": "When a machine breaks down in a factory, a technician comes to fix it. The faster they fix it, the sooner the factory is making products. But the machine manuals that technicians need to use to fix the machines are thousands of pages long. To help them find the right information faster, I built a QA (question-answer) tool that uses LLMs. But LLMs have a limit to the amount of information they can consider when answering a user’s query - this is called their context window. Being thousands of pages long, the manuals are too large for this context window. Normally AI engineers solve this by breaking the manual into pieces, finding the most relevant piece for the user’s question, then using it to answer the question, but in November 2024 Google released Gemini Pro 1.5 with a context window long enough to take in thousands of pages. So I built the tool using that model and here I’ll walk through how I built it and end with how well the tool works."
  },
  {
    "objectID": "posts/post-with-code/index.html#introduction",
    "href": "posts/post-with-code/index.html#introduction",
    "title": "Fixing factory machines faster with LLMs",
    "section": "",
    "text": "When a machine breaks down in a factory, a technician comes to fix it. The faster they fix it, the sooner the factory is making products. But the machine manuals that technicians need to use to fix the machines are thousands of pages long. To help them find the right information faster, I built a QA (question-answer) tool that uses LLMs. But LLMs have a limit to the amount of information they can consider when answering a user’s query - this is called their context window. Being thousands of pages long, the manuals are too large for this context window. Normally AI engineers solve this by breaking the manual into pieces, finding the most relevant piece for the user’s question, then using it to answer the question, but in November 2024 Google released Gemini Pro 1.5 with a context window long enough to take in thousands of pages. So I built the tool using that model and here I’ll walk through how I built it and end with how well the tool works."
  },
  {
    "objectID": "posts/post-with-code/index.html#installations",
    "href": "posts/post-with-code/index.html#installations",
    "title": "Fixing factory machines faster with LLMs",
    "section": "Installations",
    "text": "Installations\nFirst we install the necessary packages. The google-generativeai library allows us to interact with Google’s Generative AI models, and pdf2image along with poppler-utils converts PDF documents into images for processing. The imports are for utility functions later on.\n\n# Installations\n!pip install -q google-generativeai\n!apt-get install -y poppler-utils\n\n# Imports\nfrom kaggle_secrets import UserSecretsClient\nimport google.generativeai as genai\nfrom google.generativeai import caching\nfrom pdf2image import convert_from_path\nimport tempfile\nimport datetime\nimport time\nimport pandas as pd\nfrom IPython.display import display\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport math\nimport re"
  },
  {
    "objectID": "posts/post-with-code/index.html#api-keys",
    "href": "posts/post-with-code/index.html#api-keys",
    "title": "Fixing factory machines faster with LLMs",
    "section": "API Keys",
    "text": "API Keys\nNext we got API keys and set them up for our conntections to google’s API.\n\n# Configure API keys\nuser_secrets = UserSecretsClient()\napi_key = user_secrets.get_secret(\"GOOGLE_API_KEY\")\ngenai.configure(api_key=api_key)"
  },
  {
    "objectID": "posts/post-with-code/index.html#converting-pdfs-to-images",
    "href": "posts/post-with-code/index.html#converting-pdfs-to-images",
    "title": "Fixing factory machines faster with LLMs",
    "section": "Converting PDFs to Images",
    "text": "Converting PDFs to Images\nThen we converted the PDF of manual into individual images since the model cannot take PDFs as input.\n\n# Convert PDF to images\npdf_path = '/kaggle/input/manuals/manual_130.pdf'\npages = convert_from_path(pdf_path)"
  },
  {
    "objectID": "posts/post-with-code/index.html#uploading-images-to-googles-ai-platform",
    "href": "posts/post-with-code/index.html#uploading-images-to-googles-ai-platform",
    "title": "Fixing factory machines faster with LLMs",
    "section": "Uploading Images to Google’s AI Platform",
    "text": "Uploading Images to Google’s AI Platform\nWe uploaded those images using the Files API. This lets the manual to be used across different queries. It speeds things up since the files don’t need to be uploaded each time a user asks a question.\n\n# Upload pages, save file names\nuploaded_file_names = []\n\nwith tempfile.TemporaryDirectory() as temp_dir:\n    for i, page in enumerate(pages):\n        image_path = f'{temp_dir}/page_{i + 1}.jpg'\n        page.save(image_path, 'JPEG')\n\n        uploaded_file = genai.upload_file(image_path)\n        # print(f\"Uploaded file: {uploaded_file}\") # for debugging\n        uploaded_file_names.append(uploaded_file.name)"
  },
  {
    "objectID": "posts/post-with-code/index.html#adding-page-numbers",
    "href": "posts/post-with-code/index.html#adding-page-numbers",
    "title": "Fixing factory machines faster with LLMs",
    "section": "Adding Page Numbers",
    "text": "Adding Page Numbers\nOur manual did not have clear page numbers so I used a hack that would make this clear to the model.\n\n# Add context to pages\ncontext_preamble = \"\"\"\nPlease answer questions with respect to the \"ACTUAL_PAGE_NUMBER\" indices, rather than the page numbers in the manual itself.\nPlease provide the actual page numbers where the answer occurs in your response.\n\"\"\"\ncontext = [context_preamble]\nfor i, filename in enumerate(uploaded_file_names):\n    page_num = i + 1\n    context.append(f\"START OF ACTUAL PAGE NUMBER: {page_num}\")\n    page = genai.get_file(filename)\n    context.append(page)\n    context.append(f\"END OF ACTUAL PAGE NUMBER: {page_num}\\nBREAK\\n\")"
  },
  {
    "objectID": "posts/post-with-code/index.html#caching",
    "href": "posts/post-with-code/index.html#caching",
    "title": "Fixing factory machines faster with LLMs",
    "section": "Caching",
    "text": "Caching\nNext we cached the manual and invoked the model. This stores the pages that we uploaded in a place where they can be accessed faster and used more cheaply.\nIn terms of cost, you are charged by how much information you give the model, and how much text the model generates. Information that you give the model that is cached is charged at a lower rate, so if you are regularly giving the model the same information, it can be cheaper to cache this. Checkout the documentation around context caching here: https://ai.google.dev/gemini-api/docs/caching?hl=en&lang=python\n\n# Cache context, add system prompt\ncache = caching.CachedContent.create(\n    model='models/gemini-1.5-pro-002',\n    display_name='manual 130',\n    system_instruction=(\n        'You are an expert in machine repair using manuals, and your job is to answer'\n        'the user\\'s query based on the images of machine manual pages you have access to.'\n        'Ensure your answer is detailed and directly references relevant sections from the manual.'\n    ),\n    contents=context,\n    ttl=datetime.timedelta(hours=2),\n)\n\n# Construct model that uses caching\nmodel = genai.GenerativeModel.from_cached_content(cached_content=cache)"
  },
  {
    "objectID": "posts/post-with-code/index.html#prompting",
    "href": "posts/post-with-code/index.html#prompting",
    "title": "Fixing factory machines faster with LLMs",
    "section": "Prompting",
    "text": "Prompting\nNow is the main prompt, where we tell the LLM what input it gets and what output we want. There were 5 techniques we used because we noticed that the output wasn’t what we wanted, so we added each technique to solve an issue we had with the output. First we found that the model would stop looking through the manual once it found an answer, but we wanted to make sure it found all relevant pages, so we told it to scan through in 10 page sections for the whole document, then decide what pages were relevant. This is call full document chain-of-thought. Next we found that when the needed information was in a table, the model would find the right table but pull information from the wrong cell, so we told it to extract all the information from page into its own format, then extract the piece of information, which worked. Next the models are stochastic and can make mistakes, so we asked it to check its work. Next, since the model output would be more than what we want to show the user, we asked it to include the information that we wanted to show the user in XML tags so we could later use REGEX to easily extract the answer. Last, we give the model a few examples of the output we want - this is called few-shot prompting.\n\nexample_1 = \"\"\"\n&lt;example-1&gt;\nQuery: What is the CO content at idle?\n\n1. Page Scan\n\nIt looks like there are 136 pages in total, so I'll sweep through them by 10s.\n\nPages 1-10: These pages contain the cover, title page, copyright information, table of contents, and foreword. Nothing related to the CO content at idle.\n\nPages 11-20: These pages contain the index and general vehicle information like model identification and VIN locations.  Nothing related to the CO content at idle.\n\nPages 21-30: These pages continue with general information and begin engine specifications. Page 30 contains idle speed and ignition settings for the air-cooled engine with AFC (Automatic Fuel Control), but no CO content.\n\nPages 31-40: These pages continue with engine specifications. Page 31 contains idle speed and ignition settings for the California air-cooled engine with AFC, but no CO content. Page 32 has similar information for the water-cooled Digifant engine.\n\nPages 41-50: These pages cover engine assembly/removal procedures. No information on CO content.\n\nPages 51-60:  These pages cover engine - crankshaft/crankcase disassembly/assembly for the air-cooled engine. No CO content mentioned.\n\nPages 61-70: More engine-crankshaft/crankcase information for air-cooled and diesel engines.  Still no CO information.\n\nPages 71-80: Still on engine - crankshaft/crankcase information, covering both Diesel and water-cooled engines.  No CO content.\n\nPages 81-90:  More of engine - crankshaft/crankcase. No CO content.\n\nPages 91-100: More engine crankshaft and crankcase information, but nothing related to CO content at idle.\n\nPages 101-110: More engine crankshaft and crankcase information. Page 110 shows the procedure for adjusting hydraulic valve lifters on the air-cooled engine. No information on CO content.\n\nPages 111-120: Continue with cylinder head and valve drive information. Still no information about CO content.\n\nPages 121-130: Continue with engine cylinder head information, including checking compression. Still no CO information.\n\nPages 131-136: There seems to be nothing related to my query in this range either.\n----\n\n2. Extraction\nAfter looking through all pages, the idle CO content looks to be on page 44. It looks like this data appears in a table, so I'll extract the table first.\n\n| **Technical Data/Specified Values** | **Details**                                      |\n|-------------------------------------|--------------------------------------------------|\n| **Engine Code**                     | MV                                               |\n| **Type**                            | 2.1 liter 70 kW 90 SAE net HP                    |\n| **Introduction**                    | October 1985                                     |\n| **Part No.** (Control unit)         | 025 906 222                                      |\n|-------------------------------------|--------------------------------------------------|\n| **Ignition Timing Checking Spec.**  | 3-7° before TDC                                  |\n| **Ignition Timing Adjusting Spec.** | 5 ± 1° before TDC                                |\n| **Test and adjustment conditions**  | 1 and 9                                          |\n|-------------------------------------|--------------------------------------------------|\n| **Idle Adjustment idle rpm**        | 880 ± 50 rpm                                     |\n| **Idle Adjustment CO content**      | 0.7 ± 0.4 Vol. %                                 |\n|-------------------------------------|--------------------------------------------------|\n| **Test and Adjustment Conditions**  | 1 to 6, 7, 8                                     |\n\nWith the table extracted, I can see that the idle CO content is 0.7 ± 0.4 Vol. %.\n\n3. Error Correction\nI'll double check the pages that could be relevant, but it looks like this should be the correct answer. I just double checked the values in the table,\nand it looks like 0.7 ± 0.4 Vol. % is the correct value. It looks like I only used page 44 for this, so I'll just return that.\n\n4. Final Answer\n&lt;final-answer&gt;\nThe idle CO content is 0.7 ± 0.4 Vol. %.\n&lt;/final-answer&gt;\n&lt;page-references&gt;\n44\n&lt;/page-references&gt;\n&lt;/example-1&gt;\n\"\"\"\n\nexample_2 = \"\"\"\n&lt;example-2&gt;\nQuery: Where can I find information on the Sunroof?\n\n1. Page Scan\n\nIt looks like there are 136 pages in total, so I'll sweep through them by 10s.\n\nPages 1-10: These pages are the cover, title page, copyright, table of contents, and foreword. No sunroof information.\n\nPages 11-20: The index on pages 9-18 and continuation on 20 doesn't list \"sunroof\" explicitly, but I'll keep an eye out for related terms like \"roof\" or \"top.\"\n\nPages 21-30: These pages cover general information, engine identification and some specifications. No mention of the sunroof.\n\nPages 31-40: These pages continue with engine removal and installation procedures. No sunroof information here.\n\nPages 41-50: These pages continue covering engine-related procedures. No sunroof information.\n\nPages 51-60: These pages deal with air-cooled engine components. No sunroof information.\n\nPages 61-70: Still working through the air-cooled engine section and the diesel engine section. Nothing on the sunroof.\n\nPages 71-80: More on engine crankshaft and crankcase, now including water-cooled engines. Still no sunroof.\n\nPages 81-90: Still engine-related content, but nothing about the sunroof.\n\nPages 91-100: These pages continue on crankshaft/crankcase information. Nothing related to the sunroof is present.\n\nPages 101-110: These pages cover crankshaft/crankcase information, including replacing procedures. No sunroof details.\n\nPages 111-120: Cylinder head and valve drive information is covered in these pages.  Still no mention of the sunroof.\n\nPages 121-130: More information on cylinder heads and pushrod tubes. No sunroof information.\n\nPages 131-140: Final pages related to cylinder heads.  No sunroof information is present.\n----\n\nPages 4-5 Table of Contents: It contains information on the body which contains an entry for Sunroof. This entry on Sunroof covers pages 62 to 63.\n\nPages 62-63: No information on the sunroof.\n\nPages 55-64: I'll examine this range more closely since the table of contents can be inaccurate due to the non-sequential page numbering. Pages 58 and 59 have information on the sunroof, labelled as \"Sunroof.\"\n\n\n2. Extraction \n\nPage 4 shows \"Body\" has a sub-section for \"Sunroof\" listed as pages 62-63.\n\nPage 58 and 59: Show the title of Sunroof.\n\n3. Error Correction\n\nPage 4 is the index of the manual, so it in and of itself is not relevant. Also, the index shows that the relavant pages are 62-63, but after rechecking pages 50-60, I found information on the sunroof on pages 58 and 59, titled \"Sunroof.\"\n\n4. Final Answer\n\n4. Final Answer\n&lt;final-answer&gt;\nInformation on the sunroof can be found on pages 58 and 59 of the manual.\n&lt;/final-answer&gt;\n&lt;page-references&gt;\n58, 59\n&lt;/page-references&gt;\n\"\"\"\n\ndef get_prompt(query):\n    return f\"\"\"Based on the manual pages provided, answer the following question: {query}\n\nPlease provide your response in four parts:\n1. Page Scan: Explain your reasoning process, including which pages you looked at and why. Please exhaustively check every page in the input, and talk about your thoughts about each set of 10 pages. Like, I will first look at 1-10. I see nothing related to my query here. I now processed 11-20, and so on for all of the input. There are {len(pages)} pages in total, don't forget the ones on the end!\n2. Extraction: For the given pages, extract the page contents. If the answer is in a table or diagram, extract the entire table / diagram, so that you can clearly see the data you want to extract.\n3. Error Correction: If you made a mistake, or need to look at a different page, use this space to look at that page and extract data as needed. If no errors are detected, write \"No errors detected\", and list the final list of pages that you plan on returning. \n4. Final Answer: Give the precise answer to the question, as well as the pages referenced (it is possible that the answer is simply pages).\n\nFormat your response as follows:\n1. Page Scan:\n[your comprehensive page scan here]\n\n2. Extraction:\n[your detailed extraction here]\n\n3. Error Correction:\n[your detailed error correction here]\n\n4. Final Answer\n&lt;final-answer&gt;\n[your precise prose answer here]\n&lt;/final-answer&gt;\n&lt;page-references&gt;\n[page numbers here, delimited by commas]\n&lt;/page-references&gt;\n\nHere are two example outputs for your reference, please format your response accordingly:\n&lt;begin-examples&gt;\n{example_1}\n{example_2}\n&lt;/end-examples&gt;\n    \"\"\""
  },
  {
    "objectID": "posts/post-with-code/index.html#adding-utility-functions",
    "href": "posts/post-with-code/index.html#adding-utility-functions",
    "title": "Fixing factory machines faster with LLMs",
    "section": "Adding utility Functions",
    "text": "Adding utility Functions\nNext are 3 utility functions. A utility function is code that supports the main function, but isn’t the special sauce. The first function extract_answers_from_text() extracts the final answer and page references from the model response. The second function display_selected_pages() displays the relevant pages to the user. The third function get_answer_from_manual() calls the model, then uses the 2 utility functions to give the user an answer.\nThat is how it works, next I’ll tell you what it works for then cover limitations.\n\ndef extract_answer_and_references(text):\n    \"\"\"\n    Extract the final answer and page references from the formatted text.\n\n    Args:\n        text (str): Input text containing final answer and page references in XML-like format\n\n    Returns:\n        tuple: (final_answer, page_references)\n            - final_answer (str): The extracted answer text\n            - page_references (list): List of page numbers as integers\n\n    Example:\n        &gt;&gt;&gt; text = '''&lt;final-answer&gt;The idle CO content is 0.7 ± 0.4 Vol. %.&lt;/final-answer&gt;\n        ... &lt;page-references&gt;44, 53&lt;/page-references&gt;'''\n        &gt;&gt;&gt; extract_answer_and_references(text)\n        ('The idle CO content is 0.7 ± 0.4 Vol. %.', [44, 53])\n    \"\"\"\n    # Extract final answer\n    answer_match = re.search(r\"&lt;final-answer&gt;(.*?)&lt;/final-answer&gt;\", text, re.DOTALL)\n    final_answer = answer_match.group(1).strip() if answer_match else None\n\n    # Extract page references\n    ref_match = re.search(r\"&lt;page-references&gt;(.*?)&lt;/page-references&gt;\", text, re.DOTALL)\n    page_references = []\n\n    if ref_match:\n        # Split by comma and convert to integers, handling whitespace\n        refs = ref_match.group(1).strip()\n        page_references = [\n            int(page.strip()) for page in refs.split(\",\") if page.strip().isdigit()\n        ]\n\n    if final_answer is None:\n        raise ValueError(\"No final answer found in the input text\")\n\n    return final_answer, page_references\n\ndef display_selected_pages(pages, indexes, columns=2):\n    \"\"\"\n    Displays specific pages based on their indexes, arranged in a grid with a configurable number of columns.\n    Safely handles invalid indices by skipping them.\n\n    Args:\n        pages (list): List of PIL.Image objects representing the pages of a PDF.\n        indexes (list): List of indices (can be any type) representing the pages to display.\n        columns (int): Number of columns per row (default is 2).\n    \"\"\"\n    # Validate columns\n    if columns &lt; 1:\n        raise ValueError(\"The number of columns must be at least 1.\")\n\n    # Convert and filter valid indexes\n    valid_indexes = []\n    skipped_indexes = []\n\n    for idx in indexes:\n        try:\n            # Try to convert to integer\n            int_idx = int(idx)\n            # Check if index is in valid range\n            if 0 &lt;= int_idx &lt; len(pages):\n                valid_indexes.append(int_idx)\n            else:\n                skipped_indexes.append(idx)\n        except (ValueError, TypeError):\n            # If conversion fails, add to skipped list\n            skipped_indexes.append(idx)\n\n    if skipped_indexes:\n        print(f\"Skipped invalid indexes: {skipped_indexes}\")\n\n    if not valid_indexes:\n        print(\"No valid indexes provided. Nothing to display.\")\n        return\n\n    # Calculate rows needed\n    rows = math.ceil(len(valid_indexes) / columns)\n\n    # Create a grid to display pages\n    fig, axes = plt.subplots(rows, columns, figsize=(columns * 5, rows * 7))\n    # Convert axes to 2D array if it's 1D or a single axis\n    if rows == 1 and columns == 1:\n        axes = np.array([[axes]])\n    elif rows == 1 or columns == 1:\n        axes = axes.reshape(-1, columns)\n    axes = axes.flatten()  # Flatten for easier indexing\n\n    # Iterate over valid indexes and plot\n    for i, index in enumerate(valid_indexes):\n        axes[i].imshow(pages[index - 1])  # Render the page in color\n        axes[i].axis(\"off\")  # Remove axes for cleaner display\n        axes[i].set_title(f\"Page Number: {index}\")  # Set title as page index\n\n    # Hide unused subplots\n    for j in range(len(valid_indexes), len(axes)):\n        axes[j].axis(\"off\")\n\n    plt.tight_layout()\n    plt.show()\n\nindexes_to_display = [1, 2, \"A\", -1, 5, 8]  # Example with some invalid indices\ndisplay_selected_pages(pages, indexes_to_display, columns=3)\n\ndef get_answer_from_manual(query, model=model, pages=pages):\n    \"\"\"\n    Query the model about the manual and return the answer with relevant pages.\n\n    Args:\n        model: The generative AI model instance\n        pages: List of PDF pages\n        query (str): The question to ask about the manual\n\n    Returns:\n        tuple: (answer, page_numbers, raw_response)\n            - answer (str): The extracted final answer\n            - page_numbers (list): List of relevant page numbers\n            - raw_response (str): The complete raw response from the model\n\n    Raises:\n        ValueError: If no final answer is found in the response\n    \"\"\"\n    # Format and send the prompt\n    prompt = get_prompt(query=query)\n    response = model.generate_content(contents=[prompt])\n    response_text = response.candidates[0].content.parts[0].text\n\n    # Extract answer and page numbers\n    answer, page_nums = extract_answer_and_references(response_text)\n\n    print(answer)\n    display_selected_pages(pages, page_nums)\n\n    return answer, page_nums, response_text"
  },
  {
    "objectID": "posts/post-with-code/index.html#using-the-tool",
    "href": "posts/post-with-code/index.html#using-the-tool",
    "title": "Fixing factory machines faster with LLMs",
    "section": "Using the Tool",
    "text": "Using the Tool\nBelow are examples of the tool in use.\nWhat I did here is important, you actually decide what queries you want it to handle then use that to assess how well it works. Anything a technician wants to know has a location and a format. Information in the manual has a location and a format. Sometimes a technician just wants the location, or just the information, or a specific format. So we made the tool to be able to handle these types of queries. Because the manual we were working off of had a lot of information in textual procedurals, and diagrams of exploded schematics, we\nQuery 1: Looking for location In the example below,\nWe thought through what kinds of queries a technician would have and tested the 4 most frequent query types:\nThe technician wants to know where information is in the manual The technician is looking for a procedure The technician is looking for a diagram The technician is looking for a detail Information in machine manuals is often multimodal, containing text and images. In some cases a technician doesn’t care what mode the information comes in (query types 1 and 4), in others they are looking for a specific mode (query types 2 and 3).\nIn this demonstration’s manual, the modes are procedures, data tables, and a host of visuals like exploded schematics, circuit diagrams, and troubleshooting flow charts. Since the majority of the manual’s content is procedures and exploded schematics, we focused on those examples\n\n\n\nPerformance and Limitations\nThe point of the tool is find information for technicians faster. The QA tool does well on finding the right information. The technicians I talked to say that they either have a physical manul, or use ctrl+F in a PDF to search for keywords. This tool works better than keyword search because it also lets technicians search by images or diagrams that have no associated text, that could not have been found with the normal method. On the other side, the tool takes about 1 minute to generate a response so it fails to find the information for technicians faster. One solution I am working on is RAG since there would be less information for the model to process in generating an answer, so response time should be faster. The second limitation is reliability. We have examples, but don’t know how often it is right or wrong. To address this we’ll make a bunch of example queries and correct responses in the different query types, then run the tests multiple times to score the precision and recall of the model. This will let us talk about how good the tool isand when we make changes, to what extent is has improved.\n\n\nConclusion\nMay need to write"
  },
  {
    "objectID": "posts/post-with-code/index.html#performance-and-limitations",
    "href": "posts/post-with-code/index.html#performance-and-limitations",
    "title": "Fixing factory machines faster with LLMs",
    "section": "Performance and Limitations",
    "text": "Performance and Limitations\nThe point of the tool is find information for technicians faster. The QA tool does well on finding the right information. The technicians I talked to say that they either have a physical manul, or use ctrl+F in a PDF to search for keywords. This tool works better than keyword search because it also lets technicians search by images or diagrams that have no associated text, that could not have been found with the normal method. On the other side, the tool takes about 1 minute to generate a response so it fails to find the information for technicians faster. One solution I am working on is RAG since there would be less information for the model to process in generating an answer, so response time should be faster. The second limitation is reliability. We have examples, but don’t know how often it is right or wrong. To address this we’ll make a bunch of example queries and correct responses in the different query types, then run the tests multiple times to score the precision and recall of the model. This will let us talk about how good the tool isand when we make changes, to what extent is has improved."
  },
  {
    "objectID": "posts/post-with-code/index.html#conclusion",
    "href": "posts/post-with-code/index.html#conclusion",
    "title": "Fixing factory machines faster with LLMs",
    "section": "Conclusion",
    "text": "Conclusion\nMay need to write"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blog",
    "section": "",
    "text": "Fixing factory machines faster with LLMs\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJan 15, 2025\n\n\nMichael Hewlett\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJan 12, 2025\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  }
]