{
  "hash": "0454139e8efb32c5a500854877a3371c",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Fixing factory machines faster with LLMs\"\nauthor: \"Michael Hewlett\"\ndate: \"2025-01-15\"\ncategories: [code, analysis]\nexecute: \n  eval: false\n---\n\n\n## Introduction\nWhen a machine breaks down in a factory, a technician comes to fix it. The faster they fix it, the sooner the factory is making products. But the machine manuals that technicians need to use to fix the machines are <u>thousands</u> of pages long! To help them find the right information faster, my friend and I built a QA (question-answer) tool that uses LLMs. \n\nBut LLMs have a limit to the amount of information they can consider when answering a user’s query - this is called their *context window*. Being many, many pages long, the manuals are too large for this context window. Normally AI engineers solve this by breaking the manual into pieces, finding the most relevant piece for the user’s question, then using it to answer the question (this is called *retrieval augment generation* or *RAG*), but in October 2024, Google released Gemini Pro 1.5 with a context window long enough to take in thousands of pages. So my friend and I built the tool using that model. In the rest of this post I’ll walk through how we worked with Google's APIs to build the tool, show how it performs on a few queries, and end with limitations and next steps for the project. If you prefer videos, we cover the same content in this one: <https://youtu.be/m8Gdi8UseM0>\n\n## Installations\nFirst we installed the necessary packages. The google-generativeai library allows us to interact with Google's generative AI models, and pdf2image along with poppler-utils converts PDF documents into images for processing. The other imports are for utility functions later on.\n\n::: {#0bea3bef .cell execution_count=1}\n``` {.python .cell-code}\n# Installations\n!pip install -q google-generativeai\n!apt-get install -y poppler-utils\n\n# Imports\nfrom kaggle_secrets import UserSecretsClient\nimport google.generativeai as genai\nfrom google.generativeai import caching\nfrom pdf2image import convert_from_path\nimport tempfile\nimport datetime\nimport time\nimport pandas as pd\nfrom IPython.display import display\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport math\nimport re\n```\n:::\n\n\n## API Keys\nNext we got API keys from Google and set them up. To get a Gemini API key, check out the documentation here: <https://ai.google.dev/gemini-api/docs>.\n\n::: {#b9f4a069 .cell execution_count=2}\n``` {.python .cell-code}\n# Configure API keys\nuser_secrets = UserSecretsClient()\napi_key = user_secrets.get_secret(\"GOOGLE_API_KEY\")\ngenai.configure(api_key=api_key)\n```\n:::\n\n\n## Converting PDFs to Images\nThen we converted the PDF of the manual into individual images since the model cannot take PDFs as input.\n\n::: {#bd227191 .cell execution_count=3}\n``` {.python .cell-code}\n# Convert PDF to images\npdf_path = '/kaggle/input/manuals/manual_130.pdf'\npages = convert_from_path(pdf_path)\n```\n:::\n\n\n## Uploading Images\nWe uploaded those images to Google's AI platform using the Files API. Uploading is a necessary step for caching, which I'll cover in a sec.\n\n::: {#2d191852 .cell execution_count=4}\n``` {.python .cell-code}\n# Upload pages, save file names\nuploaded_file_names = []\n\nwith tempfile.TemporaryDirectory() as temp_dir:\n    for i, page in enumerate(pages):\n        image_path = f'{temp_dir}/page_{i + 1}.jpg'\n        page.save(image_path, 'JPEG')\n\n        uploaded_file = genai.upload_file(image_path)\n        # print(f\"Uploaded file: {uploaded_file}\") # for debugging\n        uploaded_file_names.append(uploaded_file.name)\n```\n:::\n\n\n## Adding Page Numbers\nOur manual did not have clear page numbers so we used a hack that tells the model what page numbers to associate with what pages.\n\n::: {#28317414 .cell execution_count=5}\n``` {.python .cell-code}\n# Add context to pages\ncontext_preamble = \"\"\"\nPlease answer questions with respect to the \"ACTUAL_PAGE_NUMBER\" indices, rather than the page numbers in the manual itself.\nPlease provide the actual page numbers where the answer occurs in your response.\n\"\"\"\ncontext = [context_preamble]\nfor i, filename in enumerate(uploaded_file_names):\n    page_num = i + 1\n    context.append(f\"START OF ACTUAL PAGE NUMBER: {page_num}\")\n    page = genai.get_file(filename)\n    context.append(page)\n    context.append(f\"END OF ACTUAL PAGE NUMBER: {page_num}\\nBREAK\\n\")\n```\n:::\n\n\n## Caching\nNext we cached the manual and initialized the model. Caching makes a copy of the uploaded images more readily available to the model so that answers are generated faster.\n\nCaching also brings down the cost. Generally with API calls to LLMs, you are charged by how much information you give the model, and how much text the model generates. Information that you give the model that is cached is charged at a lower rate, so if a big part of the information you are regularly giving the model is the same, it can be cheaper to cache this. \n\nIn our case, each time a user asks a question, we're giving the model that question AND the pages of the manual. Since the user's question changes every time, we can't cache it, but the manual pages don't change, so caching makes sense there. \n\nFor more on context caching, checkout the documentation here: <https://ai.google.dev/gemini-api/docs/caching?hl=en&lang=python>\n\n::: {#4a518041 .cell execution_count=6}\n``` {.python .cell-code}\n# Cache context, add system prompt\ncache = caching.CachedContent.create(\n    model='models/gemini-1.5-pro-002',\n    display_name='manual 130',\n    system_instruction=(\n        'You are an expert in machine repair using manuals, and your job is to answer'\n        'the user\\'s query based on the images of machine manual pages you have access to.'\n        'Ensure your answer is detailed and directly references relevant sections from the manual.'\n    ),\n    contents=context,\n    ttl=datetime.timedelta(hours=2),\n)\n\n# Construct model that uses caching\nmodel = genai.GenerativeModel.from_cached_content(cached_content=cache)\n```\n:::\n\n\n## Prompting\nThen we wrote a prompt that would pass the user's question to the model, along with instructions on how to process all the input and what output we would want the model to generate. \n\nWe built this prompt by first asking the model example questions (e.g. \"Where can I find information on rear brake lights?\"), then used those moments when then model's response was incorrect to modify the instructions in our prompt in order to get closer to the desired output. This process is called *prompt engineering*.\n\nIn this process we found 5 problems and used 5 common prompting techniques to address those.\n\nFirst we found that the model would stop looking through the manual once it found an answer, but we wanted to make sure it found all relevant pages, so we told it to scan through the whole document in 10 page sections, then decide what pages were relevant. This is called *full document chain-of-thought*. \n\nSecond we found that when the needed information was in a table, the model would find the right table but pull information from the wrong cell, so we told it to first extract all the information from page into its own format, then use its copy of the information to answer the question. \n\nThird, these models are stochastic and can make mistakes, so we asked it to check its work. \n\nFourth, since the model output would include its full document chain of thought and other irrelevant information, we asked it to include the information that we wanted to show the user in XML tags so we could later use REGEX to easily extract the answer. \n\nFifth, we gave the model a few examples of the output we wanted. This technique is called *few-shot prompting*. The code below starts with those examples because they are then read into the main prompt. You can see from the length of the example outputs why we would want to just extract the key information for the user.\n\nExample Output for Few Shot Prompting #1\n\n::: {#bd38cc6d .cell execution_count=7}\n``` {.python .cell-code}\nexample_1 = \"\"\"\n<example-1>\nQuery: What is the CO content at idle?\n\n1. Page Scan\n\nIt looks like there are 136 pages in total, so I'll sweep through them by 10s.\n\nPages 1-10: These pages contain the cover, title page, copyright information, table of contents, and foreword. Nothing related to the CO content at idle.\n\nPages 11-20: These pages contain the index and general vehicle information like model identification and VIN locations.  Nothing related to the CO content at idle.\n\nPages 21-30: These pages continue with general information and begin engine specifications. Page 30 contains idle speed and ignition settings for the air-cooled engine with AFC (Automatic Fuel Control), but no CO content.\n\nPages 31-40: These pages continue with engine specifications. Page 31 contains idle speed and ignition settings for the California air-cooled engine with AFC, but no CO content. Page 32 has similar information for the water-cooled Digifant engine.\n\nPages 41-50: These pages cover engine assembly/removal procedures. No information on CO content.\n\nPages 51-60:  These pages cover engine - crankshaft/crankcase disassembly/assembly for the air-cooled engine. No CO content mentioned.\n\nPages 61-70: More engine-crankshaft/crankcase information for air-cooled and diesel engines.  Still no CO information.\n\nPages 71-80: Still on engine - crankshaft/crankcase information, covering both Diesel and water-cooled engines.  No CO content.\n\nPages 81-90:  More of engine - crankshaft/crankcase. No CO content.\n\nPages 91-100: More engine crankshaft and crankcase information, but nothing related to CO content at idle.\n\nPages 101-110: More engine crankshaft and crankcase information. Page 110 shows the procedure for adjusting hydraulic valve lifters on the air-cooled engine. No information on CO content.\n\nPages 111-120: Continue with cylinder head and valve drive information. Still no information about CO content.\n\nPages 121-130: Continue with engine cylinder head information, including checking compression. Still no CO information.\n\nPages 131-136: There seems to be nothing related to my query in this range either.\n----\n\n2. Extraction\nAfter looking through all pages, the idle CO content looks to be on page 44. It looks like this data appears in a table, so I'll extract the table first.\n\n| **Technical Data/Specified Values** | **Details**                                      |\n|-------------------------------------|--------------------------------------------------|\n| **Engine Code**                     | MV                                               |\n| **Type**                            | 2.1 liter 70 kW 90 SAE net HP                    |\n| **Introduction**                    | October 1985                                     |\n| **Part No.** (Control unit)         | 025 906 222                                      |\n|-------------------------------------|--------------------------------------------------|\n| **Ignition Timing Checking Spec.**  | 3-7° before TDC                                  |\n| **Ignition Timing Adjusting Spec.** | 5 ± 1° before TDC                                |\n| **Test and adjustment conditions**  | 1 and 9                                          |\n|-------------------------------------|--------------------------------------------------|\n| **Idle Adjustment idle rpm**        | 880 ± 50 rpm                                     |\n| **Idle Adjustment CO content**      | 0.7 ± 0.4 Vol. %                                 |\n|-------------------------------------|--------------------------------------------------|\n| **Test and Adjustment Conditions**  | 1 to 6, 7, 8                                     |\n\nWith the table extracted, I can see that the idle CO content is 0.7 ± 0.4 Vol. %.\n\n3. Error Correction\nI'll double check the pages that could be relevant, but it looks like this should be the correct answer. I just double checked the values in the table,\nand it looks like 0.7 ± 0.4 Vol. % is the correct value. It looks like I only used page 44 for this, so I'll just return that.\n\n4. Final Answer\n<final-answer>\nThe idle CO content is 0.7 ± 0.4 Vol. %.\n</final-answer>\n<page-references>\n44\n</page-references>\n</example-1>\n\"\"\"\n```\n:::\n\n\nExample Output for Few Shot Prompting #2\n\n::: {#3fa6862d .cell execution_count=8}\n``` {.python .cell-code}\nexample_2 = \"\"\"\n<example-2>\nQuery: Where can I find information on the Sunroof?\n\n1. Page Scan\n\nIt looks like there are 136 pages in total, so I'll sweep through them by 10s.\n\nPages 1-10: These pages are the cover, title page, copyright, table of contents, and foreword. No sunroof information.\n\nPages 11-20: The index on pages 9-18 and continuation on 20 doesn't list \"sunroof\" explicitly, but I'll keep an eye out for related terms like \"roof\" or \"top.\"\n\nPages 21-30: These pages cover general information, engine identification and some specifications. No mention of the sunroof.\n\nPages 31-40: These pages continue with engine removal and installation procedures. No sunroof information here.\n\nPages 41-50: These pages continue covering engine-related procedures. No sunroof information.\n\nPages 51-60: These pages deal with air-cooled engine components. No sunroof information.\n\nPages 61-70: Still working through the air-cooled engine section and the diesel engine section. Nothing on the sunroof.\n\nPages 71-80: More on engine crankshaft and crankcase, now including water-cooled engines. Still no sunroof.\n\nPages 81-90: Still engine-related content, but nothing about the sunroof.\n\nPages 91-100: These pages continue on crankshaft/crankcase information. Nothing related to the sunroof is present.\n\nPages 101-110: These pages cover crankshaft/crankcase information, including replacing procedures. No sunroof details.\n\nPages 111-120: Cylinder head and valve drive information is covered in these pages.  Still no mention of the sunroof.\n\nPages 121-130: More information on cylinder heads and pushrod tubes. No sunroof information.\n\nPages 131-140: Final pages related to cylinder heads.  No sunroof information is present.\n----\n\nPages 4-5 Table of Contents: It contains information on the body which contains an entry for Sunroof. This entry on Sunroof covers pages 62 to 63.\n\nPages 62-63: No information on the sunroof.\n\nPages 55-64: I'll examine this range more closely since the table of contents can be inaccurate due to the non-sequential page numbering. Pages 58 and 59 have information on the sunroof, labelled as \"Sunroof.\"\n\n\n2. Extraction \n\nPage 4 shows \"Body\" has a sub-section for \"Sunroof\" listed as pages 62-63.\n\nPage 58 and 59: Show the title of Sunroof.\n\n3. Error Correction\n\nPage 4 is the index of the manual, so it in and of itself is not relevant. Also, the index shows that the relavant pages are 62-63, but after rechecking pages 50-60, I found information on the sunroof on pages 58 and 59, titled \"Sunroof.\"\n\n4. Final Answer\n\n4. Final Answer\n<final-answer>\nInformation on the sunroof can be found on pages 58 and 59 of the manual.\n</final-answer>\n<page-references>\n58, 59\n</page-references>\n\"\"\"\n```\n:::\n\n\nMain Prompt\n\n::: {#3327b847 .cell execution_count=9}\n``` {.python .cell-code}\ndef get_prompt(query):\n    return f\"\"\"Based on the manual pages provided, answer the following question: {query}\n\nPlease provide your response in four parts:\n1. Page Scan: Explain your reasoning process, including which pages you looked at and why. Please exhaustively check every page in the input, and talk about your thoughts about each set of 10 pages. Like, I will first look at 1-10. I see nothing related to my query here. I now processed 11-20, and so on for all of the input. There are {len(pages)} pages in total, don't forget the ones on the end!\n2. Extraction: For the given pages, extract the page contents. If the answer is in a table or diagram, extract the entire table / diagram, so that you can clearly see the data you want to extract.\n3. Error Correction: If you made a mistake, or need to look at a different page, use this space to look at that page and extract data as needed. If no errors are detected, write \"No errors detected\", and list the final list of pages that you plan on returning. \n4. Final Answer: Give the precise answer to the question, as well as the pages referenced (it is possible that the answer is simply pages).\n\nFormat your response as follows:\n1. Page Scan:\n[your comprehensive page scan here]\n\n2. Extraction:\n[your detailed extraction here]\n\n3. Error Correction:\n[your detailed error correction here]\n\n4. Final Answer\n<final-answer>\n[your precise prose answer here]\n</final-answer>\n<page-references>\n[page numbers here, delimited by commas]\n</page-references>\n\nHere are two example outputs for your reference, please format your response accordingly:\n<begin-examples>\n{example_1}\n{example_2}\n</end-examples>\n    \"\"\"\n```\n:::\n\n\n## Adding Utility Functions\nNext we wrote 3 utility functions. A *utility function* is code that supports the main function, similar to how utilities like electricity support the main function of a coffee shop. The first function extract_answers_from_text() extracts the final answer and page references from the model response. The second function display_selected_pages() displays the relevant pages to the user. The third function get_answer_from_manual() calls the model, then uses the other 2 utility functions to answer the user's question.\n\nUtility Function #1\n\n::: {#5626e4bd .cell execution_count=10}\n``` {.python .cell-code}\ndef extract_answer_and_references(text):\n    \"\"\"\n    Extract the final answer and page references from the formatted text.\n\n    Args:\n        text (str): Input text containing final answer and page references in XML-like format\n\n    Returns:\n        tuple: (final_answer, page_references)\n            - final_answer (str): The extracted answer text\n            - page_references (list): List of page numbers as integers\n\n    Example:\n        >>> text = '''<final-answer>The idle CO content is 0.7 ± 0.4 Vol. %.</final-answer>\n        ... <page-references>44, 53</page-references>'''\n        >>> extract_answer_and_references(text)\n        ('The idle CO content is 0.7 ± 0.4 Vol. %.', [44, 53])\n    \"\"\"\n    # Extract final answer\n    answer_match = re.search(r\"<final-answer>(.*?)</final-answer>\", text, re.DOTALL)\n    final_answer = answer_match.group(1).strip() if answer_match else None\n\n    # Extract page references\n    ref_match = re.search(r\"<page-references>(.*?)</page-references>\", text, re.DOTALL)\n    page_references = []\n\n    if ref_match:\n        # Split by comma and convert to integers, handling whitespace\n        refs = ref_match.group(1).strip()\n        page_references = [\n            int(page.strip()) for page in refs.split(\",\") if page.strip().isdigit()\n        ]\n\n    if final_answer is None:\n        raise ValueError(\"No final answer found in the input text\")\n\n    return final_answer, page_references\n```\n:::\n\n\nUtility Function #2\n\n::: {#d4ff834f .cell execution_count=11}\n``` {.python .cell-code}\ndef display_selected_pages(pages, indexes, columns=2):\n    \"\"\"\n    Displays specific pages based on their indexes, arranged in a grid with a configurable number of columns.\n    Safely handles invalid indices by skipping them.\n\n    Args:\n        pages (list): List of PIL.Image objects representing the pages of a PDF.\n        indexes (list): List of indices (can be any type) representing the pages to display.\n        columns (int): Number of columns per row (default is 2).\n    \"\"\"\n    # Validate columns\n    if columns < 1:\n        raise ValueError(\"The number of columns must be at least 1.\")\n\n    # Convert and filter valid indexes\n    valid_indexes = []\n    skipped_indexes = []\n\n    for idx in indexes:\n        try:\n            # Try to convert to integer\n            int_idx = int(idx)\n            # Check if index is in valid range\n            if 0 <= int_idx < len(pages):\n                valid_indexes.append(int_idx)\n            else:\n                skipped_indexes.append(idx)\n        except (ValueError, TypeError):\n            # If conversion fails, add to skipped list\n            skipped_indexes.append(idx)\n\n    if skipped_indexes:\n        print(f\"Skipped invalid indexes: {skipped_indexes}\")\n\n    if not valid_indexes:\n        print(\"No valid indexes provided. Nothing to display.\")\n        return\n\n    # Calculate rows needed\n    rows = math.ceil(len(valid_indexes) / columns)\n\n    # Create a grid to display pages\n    fig, axes = plt.subplots(rows, columns, figsize=(columns * 5, rows * 7))\n    # Convert axes to 2D array if it's 1D or a single axis\n    if rows == 1 and columns == 1:\n        axes = np.array([[axes]])\n    elif rows == 1 or columns == 1:\n        axes = axes.reshape(-1, columns)\n    axes = axes.flatten()  # Flatten for easier indexing\n\n    # Iterate over valid indexes and plot\n    for i, index in enumerate(valid_indexes):\n        axes[i].imshow(pages[index - 1])  # Render the page in color\n        axes[i].axis(\"off\")  # Remove axes for cleaner display\n        axes[i].set_title(f\"Page Number: {index}\")  # Set title as page index\n\n    # Hide unused subplots\n    for j in range(len(valid_indexes), len(axes)):\n        axes[j].axis(\"off\")\n\n    plt.tight_layout()\n    plt.show()\n\nindexes_to_display = [1, 2, \"A\", -1, 5, 8]  # Example with some invalid indices\ndisplay_selected_pages(pages, indexes_to_display, columns=3)\n```\n:::\n\n\nUtility Function #3\n\n::: {#f70b9505 .cell execution_count=12}\n``` {.python .cell-code}\ndef get_answer_from_manual(query, model=model, pages=pages):\n    \"\"\"\n    Query the model about the manual and return the answer with relevant pages.\n\n    Args:\n        model: The generative AI model instance\n        pages: List of PDF pages\n        query (str): The question to ask about the manual\n\n    Returns:\n        tuple: (answer, page_numbers, raw_response)\n            - answer (str): The extracted final answer\n            - page_numbers (list): List of relevant page numbers\n            - raw_response (str): The complete raw response from the model\n\n    Raises:\n        ValueError: If no final answer is found in the response\n    \"\"\"\n    # Format and send the prompt\n    prompt = get_prompt(query=query)\n    response = model.generate_content(contents=[prompt])\n    response_text = response.candidates[0].content.parts[0].text\n\n    # Extract answer and page numbers\n    answer, page_nums = extract_answer_and_references(response_text)\n\n    print(answer)\n    display_selected_pages(pages, page_nums)\n\n    return answer, page_nums, response_text\n```\n:::\n\n\nThat is how we worked with Google's APIs to build the tool. Next I'll show how it performs on a few queries.\n\n## Using the Tool\nWe thought through what kinds of queries a technician would ask and tested the 4 most frequent query types:\n\n1. The technician wants to know where information is in the manual\n\n2. The technician is looking for a procedure\n\n3. The technician is looking for a diagram\n\n4. The technician is looking for a detail\n\nInformation in machine manuals is often multimodal, e.g. containing text and images. In some cases a technician doesn't care what format the information comes in (query types 1 and 4), in others they are looking for a specific format (query types 2 and 3).\n\nIn our example manual, the formats are procedures, data tables, and a host of visuals like exploded schematics, circuit diagrams, and troubleshooting flow charts. Since the majority of the manual's content is procedures and exploded schematics, we focused on those examples.\n\nExample #1\n\n```{=html}\n<iframe src=\"https://www.kaggle.com/embed/mhewlett/google-gemini-long-context?cellIds=31&kernelSessionId=210373422\" height=\"800\" style=\"margin: 0 auto; width: 100%; max-width: 950px;\" frameborder=\"0\" scrolling=\"auto\" title=\"Google - Gemini Long Context\"></iframe>\n```\n\n\nExample #2\n\n```{=html}\n<iframe src=\"https://www.kaggle.com/embed/mhewlett/google-gemini-long-context?cellIds=32&kernelSessionId=210373422\" height=\"800\" style=\"margin: 0 auto; width: 100%; max-width: 950px;\" frameborder=\"0\" scrolling=\"auto\" title=\"Google - Gemini Long Context\"></iframe>\n```\n\n\nExample #3\n\n```{=html}\n<iframe src=\"https://www.kaggle.com/embed/mhewlett/google-gemini-long-context?cellIds=33&kernelSessionId=210373422\" height=\"700\" style=\"margin: 0 auto; width: 100%; max-width: 950px;\" frameborder=\"0\" scrolling=\"auto\" title=\"Google - Gemini Long Context\"></iframe>\n```\n\n\nExample #4\n\n```{=html}\n<iframe src=\"https://www.kaggle.com/embed/mhewlett/google-gemini-long-context?cellIds=34&kernelSessionId=210373422\" height=\"700\" style=\"margin: 0 auto; width: 100%; max-width: 950px;\" frameborder=\"0\" scrolling=\"auto\" title=\"Google - Gemini Long Context\"></iframe>\n```\n\n\n## Performance and Limitations\n\nThe point of the tool was find the right information for technicians, and find it faster. The technician's default solution is either to go through a physical manual, or use CTRL+F to do keyword search on a PDF. This tool does better than keyword search for finding information because it also lets technicians search by describing images or diagrams that have no associated text. This kind of content just can't be found with CTRL+F. In terms of speed, the tool takes about 1 minute to generate a response. That is too slow. One solution to this limitation would be to use RAG - there would be fewer pages from the manual given to the model, so response time should be faster. \n\nSeparate from accuracy and speed, this tool lacks metrics. Case in point, we used a handful of example queries and manually reviewed the output to judge whether the answers were correct. So one of the next steps in developing this tool is to generate a large number of example queries and correct responses, then run those queries several times to compare the model responses to the correct responses. From this we could calculate metrics like precision and recall, then use those as benchmarks to evaluate whether changes in prompts or models improve the tool and by how much.\n\n## Conclusion\nUsing the long context window of Google's Gemini Pro 1.5, we were able to build a tool to answer technicians' questions from information in a machine manual. The tool could produce accurate answers and find information that technicians' current solutions can't, but it's not fast enough to be useful (yet). \n\nThe key stages of developing with Google's latest AI model were getting API keys, uploading the context using the Files API, caching the context, initializing the model, and then calling it. The key method we used to improve model accuracy was prompt engineering, and the key method we used to improve model speed was context caching. Our solution also used code that converted the context into the right input format, and defined functions to process the model's output before displaying it to the user. \n\nFuture iterations of the project will experiment with RAG to improve speed, and robust testing to assess accuracy.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}