---
title: "Fixing factory machines faster with LLMs"
author: "Michael Hewlett"
date: "2025-01-15"
categories: [code, analysis]
execute: 
  eval: false
---

## Introduction
When a machine breaks down in a factory, a technician comes to fix it. The faster they fix it, the sooner the factory is making products. But the machine manuals that technicians need to use to fix the machines are <u>thousands</u> of pages long! To help them find the right information faster, my friend and I built a QA (question-answer) tool that uses LLMs. 

But LLMs have a limit to the amount of information they can consider when answering a user’s query - this is called their *context window*. Being many, many pages long, the manuals are too large for this context window. Normally AI engineers solve this by breaking the manual into pieces, finding the most relevant piece for the user’s question, then using it to answer the question (this is called *retrieval augment generation* or *RAG*), but in October 2024, Google released Gemini Pro 1.5 with a context window long enough to take in thousands of pages. So my friend and I built the tool using that model. In the rest of this post I’ll walk through how we worked with Google's APIs to build the tool, show how it performs on a few queries, and end with limitations and next steps for the project.

## Installations
First we installed the necessary packages. The google-generativeai library allows us to interact with Google's generative AI models, and pdf2image along with poppler-utils converts PDF documents into images for processing. The other imports are for utility functions later on.

```{python}
# Installations
!pip install -q google-generativeai
!apt-get install -y poppler-utils

# Imports
from kaggle_secrets import UserSecretsClient
import google.generativeai as genai
from google.generativeai import caching
from pdf2image import convert_from_path
import tempfile
import datetime
import time
import pandas as pd
from IPython.display import display
import matplotlib.pyplot as plt
import numpy as np
import math
import re
```


## API Keys
Next we got API keys from Google and set them up. To get a Gemini API key, check out the documentation here: <https://ai.google.dev/gemini-api/docs>.

```{python}
# Configure API keys
user_secrets = UserSecretsClient()
api_key = user_secrets.get_secret("GOOGLE_API_KEY")
genai.configure(api_key=api_key)
```

## Converting PDFs to Images
Then we converted the PDF of the manual into individual images since the model cannot take PDFs as input.

```{python}
# Convert PDF to images
pdf_path = '/kaggle/input/manuals/manual_130.pdf'
pages = convert_from_path(pdf_path)
```

## Uploading Images
We uploaded those images to Google's AI platform using the Files API. Uploading is a necessary step for caching, which I'll cover in a sec.

```{python}
# Upload pages, save file names
uploaded_file_names = []

with tempfile.TemporaryDirectory() as temp_dir:
    for i, page in enumerate(pages):
        image_path = f'{temp_dir}/page_{i + 1}.jpg'
        page.save(image_path, 'JPEG')

        uploaded_file = genai.upload_file(image_path)
        # print(f"Uploaded file: {uploaded_file}") # for debugging
        uploaded_file_names.append(uploaded_file.name)
```

## Adding Page Numbers
Our manual did not have clear page numbers so we used a hack that tells the model what page numbers to associate with what pages.

```{python}
# Add context to pages
context_preamble = """
Please answer questions with respect to the "ACTUAL_PAGE_NUMBER" indices, rather than the page numbers in the manual itself.
Please provide the actual page numbers where the answer occurs in your response.
"""
context = [context_preamble]
for i, filename in enumerate(uploaded_file_names):
    page_num = i + 1
    context.append(f"START OF ACTUAL PAGE NUMBER: {page_num}")
    page = genai.get_file(filename)
    context.append(page)
    context.append(f"END OF ACTUAL PAGE NUMBER: {page_num}\nBREAK\n")
```

## Caching
Next we cached the manual and initialized the model. Caching makes a copy of the uploaded images more readily available to the model so that answers are generated faster.

Caching also brings down the cost. Generally with API calls to LLMs, you are charged by how much information you give the model, and how much text the model generates. Information that you give the model that is cached is charged at a lower rate, so if a big part of the information you are regularly giving the model is the same, it can be cheaper to cache this. 

In our case, each time a user asks a question, we're giving the model that question AND the pages of the manual. Since the user's question changes every time, we can't cache it, but the manual pages don't change, so caching makes sense there. 

For more on context caching, checkout the documentation here: <https://ai.google.dev/gemini-api/docs/caching?hl=en&lang=python>

```{python}
# Cache context, add system prompt
cache = caching.CachedContent.create(
    model='models/gemini-1.5-pro-002',
    display_name='manual 130',
    system_instruction=(
        'You are an expert in machine repair using manuals, and your job is to answer'
        'the user\'s query based on the images of machine manual pages you have access to.'
        'Ensure your answer is detailed and directly references relevant sections from the manual.'
    ),
    contents=context,
    ttl=datetime.timedelta(hours=2),
)

# Construct model that uses caching
model = genai.GenerativeModel.from_cached_content(cached_content=cache)
```

## Prompting
Then we wrote a prompt that would pass the user's question to the model, along with instructions on how to process all the input and what output we would want the model to generate. 

We built this prompt by first asking the model example questions (e.g. "Where can I find information on rear brake lights?"), then used those moments when then model's response was incorrect to modify the instructions in our prompt in order to get closer to the desired output. This process is called *prompt engineering*.

In this process we found 5 problems and used 5 common prompting techniques to address those.

First we found that the model would stop looking through the manual once it found an answer, but we wanted to make sure it found all relevant pages, so we told it to scan through the whole document in 10 page sections, then decide what pages were relevant. This is called *full document chain-of-thought*. 

Second we found that when the needed information was in a table, the model would find the right table but pull information from the wrong cell, so we told it to first extract all the information from page into its own format, then use its copy of the information to answer the question. 

Third, these models are stochastic and can make mistakes, so we asked it to check its work. 

Fourth, since the model output would include its full document chain of thought and other irrelevant information, we asked it to include the information that we wanted to show the user in XML tags so we could later use REGEX to easily extract the answer. 

Fifth, we gave the model a few examples of the output we wanted. This technique is called *few-shot prompting*. The code below starts with those examples because they are then read into the main prompt. You can see from the length of the example outputs why we would want to just extract the key information for the user.

Example Output for Few Shot Prompting #1
```{python}
example_1 = """
<example-1>
Query: What is the CO content at idle?

1. Page Scan

It looks like there are 136 pages in total, so I'll sweep through them by 10s.

Pages 1-10: These pages contain the cover, title page, copyright information, table of contents, and foreword. Nothing related to the CO content at idle.

Pages 11-20: These pages contain the index and general vehicle information like model identification and VIN locations.  Nothing related to the CO content at idle.

Pages 21-30: These pages continue with general information and begin engine specifications. Page 30 contains idle speed and ignition settings for the air-cooled engine with AFC (Automatic Fuel Control), but no CO content.

Pages 31-40: These pages continue with engine specifications. Page 31 contains idle speed and ignition settings for the California air-cooled engine with AFC, but no CO content. Page 32 has similar information for the water-cooled Digifant engine.

Pages 41-50: These pages cover engine assembly/removal procedures. No information on CO content.

Pages 51-60:  These pages cover engine - crankshaft/crankcase disassembly/assembly for the air-cooled engine. No CO content mentioned.

Pages 61-70: More engine-crankshaft/crankcase information for air-cooled and diesel engines.  Still no CO information.

Pages 71-80: Still on engine - crankshaft/crankcase information, covering both Diesel and water-cooled engines.  No CO content.

Pages 81-90:  More of engine - crankshaft/crankcase. No CO content.

Pages 91-100: More engine crankshaft and crankcase information, but nothing related to CO content at idle.

Pages 101-110: More engine crankshaft and crankcase information. Page 110 shows the procedure for adjusting hydraulic valve lifters on the air-cooled engine. No information on CO content.

Pages 111-120: Continue with cylinder head and valve drive information. Still no information about CO content.

Pages 121-130: Continue with engine cylinder head information, including checking compression. Still no CO information.

Pages 131-136: There seems to be nothing related to my query in this range either.
----

2. Extraction
After looking through all pages, the idle CO content looks to be on page 44. It looks like this data appears in a table, so I'll extract the table first.

| **Technical Data/Specified Values** | **Details**                                      |
|-------------------------------------|--------------------------------------------------|
| **Engine Code**                     | MV                                               |
| **Type**                            | 2.1 liter 70 kW 90 SAE net HP                    |
| **Introduction**                    | October 1985                                     |
| **Part No.** (Control unit)         | 025 906 222                                      |
|-------------------------------------|--------------------------------------------------|
| **Ignition Timing Checking Spec.**  | 3-7° before TDC                                  |
| **Ignition Timing Adjusting Spec.** | 5 ± 1° before TDC                                |
| **Test and adjustment conditions**  | 1 and 9                                          |
|-------------------------------------|--------------------------------------------------|
| **Idle Adjustment idle rpm**        | 880 ± 50 rpm                                     |
| **Idle Adjustment CO content**      | 0.7 ± 0.4 Vol. %                                 |
|-------------------------------------|--------------------------------------------------|
| **Test and Adjustment Conditions**  | 1 to 6, 7, 8                                     |

With the table extracted, I can see that the idle CO content is 0.7 ± 0.4 Vol. %.

3. Error Correction
I'll double check the pages that could be relevant, but it looks like this should be the correct answer. I just double checked the values in the table,
and it looks like 0.7 ± 0.4 Vol. % is the correct value. It looks like I only used page 44 for this, so I'll just return that.

4. Final Answer
<final-answer>
The idle CO content is 0.7 ± 0.4 Vol. %.
</final-answer>
<page-references>
44
</page-references>
</example-1>
"""
```

Example Output for Few Shot Prompting #2
```{python}
example_2 = """
<example-2>
Query: Where can I find information on the Sunroof?

1. Page Scan

It looks like there are 136 pages in total, so I'll sweep through them by 10s.

Pages 1-10: These pages are the cover, title page, copyright, table of contents, and foreword. No sunroof information.

Pages 11-20: The index on pages 9-18 and continuation on 20 doesn't list "sunroof" explicitly, but I'll keep an eye out for related terms like "roof" or "top."

Pages 21-30: These pages cover general information, engine identification and some specifications. No mention of the sunroof.

Pages 31-40: These pages continue with engine removal and installation procedures. No sunroof information here.

Pages 41-50: These pages continue covering engine-related procedures. No sunroof information.

Pages 51-60: These pages deal with air-cooled engine components. No sunroof information.

Pages 61-70: Still working through the air-cooled engine section and the diesel engine section. Nothing on the sunroof.

Pages 71-80: More on engine crankshaft and crankcase, now including water-cooled engines. Still no sunroof.

Pages 81-90: Still engine-related content, but nothing about the sunroof.

Pages 91-100: These pages continue on crankshaft/crankcase information. Nothing related to the sunroof is present.

Pages 101-110: These pages cover crankshaft/crankcase information, including replacing procedures. No sunroof details.

Pages 111-120: Cylinder head and valve drive information is covered in these pages.  Still no mention of the sunroof.

Pages 121-130: More information on cylinder heads and pushrod tubes. No sunroof information.

Pages 131-140: Final pages related to cylinder heads.  No sunroof information is present.
----

Pages 4-5 Table of Contents: It contains information on the body which contains an entry for Sunroof. This entry on Sunroof covers pages 62 to 63.

Pages 62-63: No information on the sunroof.

Pages 55-64: I'll examine this range more closely since the table of contents can be inaccurate due to the non-sequential page numbering. Pages 58 and 59 have information on the sunroof, labelled as "Sunroof."


2. Extraction 

Page 4 shows "Body" has a sub-section for "Sunroof" listed as pages 62-63.

Page 58 and 59: Show the title of Sunroof.

3. Error Correction

Page 4 is the index of the manual, so it in and of itself is not relevant. Also, the index shows that the relavant pages are 62-63, but after rechecking pages 50-60, I found information on the sunroof on pages 58 and 59, titled "Sunroof."

4. Final Answer

4. Final Answer
<final-answer>
Information on the sunroof can be found on pages 58 and 59 of the manual.
</final-answer>
<page-references>
58, 59
</page-references>
"""
```

Main Prompt
```{python}
def get_prompt(query):
    return f"""Based on the manual pages provided, answer the following question: {query}

Please provide your response in four parts:
1. Page Scan: Explain your reasoning process, including which pages you looked at and why. Please exhaustively check every page in the input, and talk about your thoughts about each set of 10 pages. Like, I will first look at 1-10. I see nothing related to my query here. I now processed 11-20, and so on for all of the input. There are {len(pages)} pages in total, don't forget the ones on the end!
2. Extraction: For the given pages, extract the page contents. If the answer is in a table or diagram, extract the entire table / diagram, so that you can clearly see the data you want to extract.
3. Error Correction: If you made a mistake, or need to look at a different page, use this space to look at that page and extract data as needed. If no errors are detected, write "No errors detected", and list the final list of pages that you plan on returning. 
4. Final Answer: Give the precise answer to the question, as well as the pages referenced (it is possible that the answer is simply pages).

Format your response as follows:
1. Page Scan:
[your comprehensive page scan here]

2. Extraction:
[your detailed extraction here]

3. Error Correction:
[your detailed error correction here]

4. Final Answer
<final-answer>
[your precise prose answer here]
</final-answer>
<page-references>
[page numbers here, delimited by commas]
</page-references>

Here are two example outputs for your reference, please format your response accordingly:
<begin-examples>
{example_1}
{example_2}
</end-examples>
    """
```

## Adding Utility Functions
Next we wrote 3 utility functions. A *utility function* is code that supports the main function, similar to how utilities like electricity support the main function of a coffee shop. The first function extract_answers_from_text() extracts the final answer and page references from the model response. The second function display_selected_pages() displays the relevant pages to the user. The third function get_answer_from_manual() calls the model, then uses the other 2 utility functions to answer the user's question.

Utility Function #1
```{python}
def extract_answer_and_references(text):
    """
    Extract the final answer and page references from the formatted text.

    Args:
        text (str): Input text containing final answer and page references in XML-like format

    Returns:
        tuple: (final_answer, page_references)
            - final_answer (str): The extracted answer text
            - page_references (list): List of page numbers as integers

    Example:
        >>> text = '''<final-answer>The idle CO content is 0.7 ± 0.4 Vol. %.</final-answer>
        ... <page-references>44, 53</page-references>'''
        >>> extract_answer_and_references(text)
        ('The idle CO content is 0.7 ± 0.4 Vol. %.', [44, 53])
    """
    # Extract final answer
    answer_match = re.search(r"<final-answer>(.*?)</final-answer>", text, re.DOTALL)
    final_answer = answer_match.group(1).strip() if answer_match else None

    # Extract page references
    ref_match = re.search(r"<page-references>(.*?)</page-references>", text, re.DOTALL)
    page_references = []

    if ref_match:
        # Split by comma and convert to integers, handling whitespace
        refs = ref_match.group(1).strip()
        page_references = [
            int(page.strip()) for page in refs.split(",") if page.strip().isdigit()
        ]

    if final_answer is None:
        raise ValueError("No final answer found in the input text")

    return final_answer, page_references
```

Utility Function #2
```{python}
def display_selected_pages(pages, indexes, columns=2):
    """
    Displays specific pages based on their indexes, arranged in a grid with a configurable number of columns.
    Safely handles invalid indices by skipping them.

    Args:
        pages (list): List of PIL.Image objects representing the pages of a PDF.
        indexes (list): List of indices (can be any type) representing the pages to display.
        columns (int): Number of columns per row (default is 2).
    """
    # Validate columns
    if columns < 1:
        raise ValueError("The number of columns must be at least 1.")

    # Convert and filter valid indexes
    valid_indexes = []
    skipped_indexes = []

    for idx in indexes:
        try:
            # Try to convert to integer
            int_idx = int(idx)
            # Check if index is in valid range
            if 0 <= int_idx < len(pages):
                valid_indexes.append(int_idx)
            else:
                skipped_indexes.append(idx)
        except (ValueError, TypeError):
            # If conversion fails, add to skipped list
            skipped_indexes.append(idx)

    if skipped_indexes:
        print(f"Skipped invalid indexes: {skipped_indexes}")

    if not valid_indexes:
        print("No valid indexes provided. Nothing to display.")
        return

    # Calculate rows needed
    rows = math.ceil(len(valid_indexes) / columns)

    # Create a grid to display pages
    fig, axes = plt.subplots(rows, columns, figsize=(columns * 5, rows * 7))
    # Convert axes to 2D array if it's 1D or a single axis
    if rows == 1 and columns == 1:
        axes = np.array([[axes]])
    elif rows == 1 or columns == 1:
        axes = axes.reshape(-1, columns)
    axes = axes.flatten()  # Flatten for easier indexing

    # Iterate over valid indexes and plot
    for i, index in enumerate(valid_indexes):
        axes[i].imshow(pages[index - 1])  # Render the page in color
        axes[i].axis("off")  # Remove axes for cleaner display
        axes[i].set_title(f"Page Number: {index}")  # Set title as page index

    # Hide unused subplots
    for j in range(len(valid_indexes), len(axes)):
        axes[j].axis("off")

    plt.tight_layout()
    plt.show()

indexes_to_display = [1, 2, "A", -1, 5, 8]  # Example with some invalid indices
display_selected_pages(pages, indexes_to_display, columns=3)
```

Utility Function #3
```{python}


def get_answer_from_manual(query, model=model, pages=pages):
    """
    Query the model about the manual and return the answer with relevant pages.

    Args:
        model: The generative AI model instance
        pages: List of PDF pages
        query (str): The question to ask about the manual

    Returns:
        tuple: (answer, page_numbers, raw_response)
            - answer (str): The extracted final answer
            - page_numbers (list): List of relevant page numbers
            - raw_response (str): The complete raw response from the model

    Raises:
        ValueError: If no final answer is found in the response
    """
    # Format and send the prompt
    prompt = get_prompt(query=query)
    response = model.generate_content(contents=[prompt])
    response_text = response.candidates[0].content.parts[0].text

    # Extract answer and page numbers
    answer, page_nums = extract_answer_and_references(response_text)

    print(answer)
    display_selected_pages(pages, page_nums)

    return answer, page_nums, response_text
```

That is how we worked with Google's APIs to build the tool. Next I'll show how it performs on a few queries.

## Using the Tool
We thought through what kinds of queries a technician would ask and tested the 4 most frequent query types:

1. The technician wants to know where information is in the manual

2. The technician is looking for a procedure

3. The technician is looking for a diagram

4. The technician is looking for a detail

Information in machine manuals is often multimodal, e.g. containing text and images. In some cases a technician doesn't care what format the information comes in (query types 1 and 4), in others they are looking for a specific format (query types 2 and 3).

In our example manual, the formats are procedures, data tables, and a host of visuals like exploded schematics, circuit diagrams, and troubleshooting flow charts. Since the majority of the manual's content is procedures and exploded schematics, we focused on those examples.

Example #1
```{=html}
<iframe src="https://www.kaggle.com/embed/mhewlett/google-gemini-long-context?cellIds=31&kernelSessionId=210373422" height="800" style="margin: 0 auto; width: 100%; max-width: 950px;" frameborder="0" scrolling="auto" title="Google - Gemini Long Context"></iframe>
```

Example #2
```{=html}
<iframe src="https://www.kaggle.com/embed/mhewlett/google-gemini-long-context?cellIds=32&kernelSessionId=210373422" height="800" style="margin: 0 auto; width: 100%; max-width: 950px;" frameborder="0" scrolling="auto" title="Google - Gemini Long Context"></iframe>
```

Example #3
```{=html}
<iframe src="https://www.kaggle.com/embed/mhewlett/google-gemini-long-context?cellIds=33&kernelSessionId=210373422" height="700" style="margin: 0 auto; width: 100%; max-width: 950px;" frameborder="0" scrolling="auto" title="Google - Gemini Long Context"></iframe>
```

Example #4
```{=html}
<iframe src="https://www.kaggle.com/embed/mhewlett/google-gemini-long-context?cellIds=34&kernelSessionId=210373422" height="700" style="margin: 0 auto; width: 100%; max-width: 950px;" frameborder="0" scrolling="auto" title="Google - Gemini Long Context"></iframe>
```

## Performance and Limitations

The point of the tool was find the right information for technicians, and find it faster. The technician's default solution is either to go through a physical manual, or use CTRL+F to do keyword search on a PDF. This tool does better than keyword search for finding information because it also lets technicians search by describing images or diagrams that have no associated text. This kind of content just can't be found with CTRL+F. In terms of speed, the tool takes about 1 minute to generate a response. That is too slow. One solution to this limitation would be to use RAG - there would be fewer pages from the manual given to the model, so response time should be faster. 

Separate from accuracy and speed, this tool lacks metrics. Case in point, we used a handful of example queries and manually reviewed the output to judge whether the answers were correct. So one of the next steps in developing this tool is to generate a large number of example queries and correct responses, then run those queries several times to compare the model responses to the correct responses. From this we could calculate metrics like precision and recall, then use those as benchmarks to evaluate whether changes in prompts or models improve the tool and by how much.

## Conclusion
Using the long context window of Google's Gemini Pro 1.5, we were able to build a tool to answer technicians' questions from information in a machine manual. The tool could produce accurate answers and find information that technicians' current solutions can't, but it's not fast enough to be useful (yet). 

The key stages of developing with Google's latest AI model were getting API keys, uploading the context using the Files API, caching the context, initializing the model, and then calling it. The key method we used to improve model accuracy was prompt engineering, and the key method we used to improve model speed was context caching. Our solution also used code that converted the context into the right input format, and defined functions to process the model's output before displaying it to the user. 

Future iterations of the project will experiment with RAG to improve speed, and robust testing to assess accuracy.