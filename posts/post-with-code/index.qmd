---
title: "Fixing factory machines faster with LLMs"
author: "Michael Hewlett"
date: "2025-01-15"
categories: [code, analysis]
image: "image.jpg"
execute: 
  eval: false
---

## Introduction
When a machine breaks down in a factory, a technician comes to fix it. The faster they fix it, the sooner the factory is making products. But the machine manuals that technicians need to use to fix the machines are thousands of pages long. To help them find the right information faster, I built a QA (question-answer) tool that uses LLMs. But LLMs have a limit to the amount of information they can consider when answering a user’s query - this is called their context window. Being thousands of pages long, the manuals are too large for this context window. Normally AI engineers solve this by breaking the manual into pieces, finding the most relevant piece for the user’s question, then using it to answer the question, but in November 2024 Google released Gemini Pro 1.5 with a context window long enough to take in thousands of pages. So I built the tool using that model and here I’ll walk through how I built it and end with how well the tool works.

## Installations
First we install the necessary packages. The google-generativeai library allows us to interact with Google's Generative AI models, and pdf2image along with poppler-utils converts PDF documents into images for processing. The imports are for utility functions later on.

```{python}
# Installations
!pip install -q google-generativeai
!apt-get install -y poppler-utils

# Imports
from kaggle_secrets import UserSecretsClient
import google.generativeai as genai
from google.generativeai import caching
from pdf2image import convert_from_path
import tempfile
import datetime
import time
import pandas as pd
from IPython.display import display
import matplotlib.pyplot as plt
import numpy as np
import math
import re
```


## API Keys
Next we got API keys and set them up for our conntections to google’s API.

```{python}
# Configure API keys
user_secrets = UserSecretsClient()
api_key = user_secrets.get_secret("GOOGLE_API_KEY")
genai.configure(api_key=api_key)
```

## Converting PDFs to Images
Then we converted the PDF of manual into individual images since the model cannot take PDFs as input.

```{python}
# Convert PDF to images
pdf_path = '/kaggle/input/manuals/manual_130.pdf'
pages = convert_from_path(pdf_path)
```

## Uploading Images to Google's AI Platform
We uploaded those images using the Files API. This lets the manual to be used across different queries. It speeds things up since the files don’t need to be uploaded each time a user asks a question.

```{python}
# Upload pages, save file names
uploaded_file_names = []

with tempfile.TemporaryDirectory() as temp_dir:
    for i, page in enumerate(pages):
        image_path = f'{temp_dir}/page_{i + 1}.jpg'
        page.save(image_path, 'JPEG')

        uploaded_file = genai.upload_file(image_path)
        # print(f"Uploaded file: {uploaded_file}") # for debugging
        uploaded_file_names.append(uploaded_file.name)
```

## Adding Page Numbers
Our manual did not have clear page numbers so I used a hack that would make this clear to the model.

```{python}
# Add context to pages
context_preamble = """
Please answer questions with respect to the "ACTUAL_PAGE_NUMBER" indices, rather than the page numbers in the manual itself.
Please provide the actual page numbers where the answer occurs in your response.
"""
context = [context_preamble]
for i, filename in enumerate(uploaded_file_names):
    page_num = i + 1
    context.append(f"START OF ACTUAL PAGE NUMBER: {page_num}")
    page = genai.get_file(filename)
    context.append(page)
    context.append(f"END OF ACTUAL PAGE NUMBER: {page_num}\nBREAK\n")
```

## Caching
Next we cached the manual and invoked the model. This stores the pages that we uploaded in a place where they can be accessed faster and used more cheaply.

In terms of cost, you are charged by how much information you give the model, and how much text the model generates. Information that you give the model that is cached is charged at a lower rate, so if you are regularly giving the model the same information, it can be cheaper to cache this. Checkout the documentation around context caching here: https://ai.google.dev/gemini-api/docs/caching?hl=en&lang=python

```{python}
# Cache context, add system prompt
cache = caching.CachedContent.create(
    model='models/gemini-1.5-pro-002',
    display_name='manual 130',
    system_instruction=(
        'You are an expert in machine repair using manuals, and your job is to answer'
        'the user\'s query based on the images of machine manual pages you have access to.'
        'Ensure your answer is detailed and directly references relevant sections from the manual.'
    ),
    contents=context,
    ttl=datetime.timedelta(hours=2),
)

# Construct model that uses caching
model = genai.GenerativeModel.from_cached_content(cached_content=cache)
```

## Prompting
Now is the main prompt, where we tell the LLM what input it gets and what output we want. There were 5 techniques we used because we noticed that the output wasn’t what we wanted, so we added each technique to solve an issue we had with the output. First we found that the model would stop looking through the manual once it found an answer, but we wanted to make sure it found all relevant pages, so we told it to scan through in 10 page sections for the whole document, then decide what pages were relevant. This is call full document chain-of-thought. Next we found that when the needed information was in a table, the model would find the right table but pull information from the wrong cell, so we told it to extract all the information from page into its own format, then extract the piece of information, which worked. Next the models are stochastic and can make mistakes, so we asked it to check its work. Next, since the model output would be more than what we want to show the user, we asked it to include the information that we wanted to show the user in XML tags so we could later use REGEX to easily extract the answer. Last, we give the model a few examples of the output we want - this is called few-shot prompting.

```{python}
example_1 = """
<example-1>
Query: What is the CO content at idle?

1. Page Scan

It looks like there are 136 pages in total, so I'll sweep through them by 10s.

Pages 1-10: These pages contain the cover, title page, copyright information, table of contents, and foreword. Nothing related to the CO content at idle.

Pages 11-20: These pages contain the index and general vehicle information like model identification and VIN locations.  Nothing related to the CO content at idle.

Pages 21-30: These pages continue with general information and begin engine specifications. Page 30 contains idle speed and ignition settings for the air-cooled engine with AFC (Automatic Fuel Control), but no CO content.

Pages 31-40: These pages continue with engine specifications. Page 31 contains idle speed and ignition settings for the California air-cooled engine with AFC, but no CO content. Page 32 has similar information for the water-cooled Digifant engine.

Pages 41-50: These pages cover engine assembly/removal procedures. No information on CO content.

Pages 51-60:  These pages cover engine - crankshaft/crankcase disassembly/assembly for the air-cooled engine. No CO content mentioned.

Pages 61-70: More engine-crankshaft/crankcase information for air-cooled and diesel engines.  Still no CO information.

Pages 71-80: Still on engine - crankshaft/crankcase information, covering both Diesel and water-cooled engines.  No CO content.

Pages 81-90:  More of engine - crankshaft/crankcase. No CO content.

Pages 91-100: More engine crankshaft and crankcase information, but nothing related to CO content at idle.

Pages 101-110: More engine crankshaft and crankcase information. Page 110 shows the procedure for adjusting hydraulic valve lifters on the air-cooled engine. No information on CO content.

Pages 111-120: Continue with cylinder head and valve drive information. Still no information about CO content.

Pages 121-130: Continue with engine cylinder head information, including checking compression. Still no CO information.

Pages 131-136: There seems to be nothing related to my query in this range either.
----

2. Extraction
After looking through all pages, the idle CO content looks to be on page 44. It looks like this data appears in a table, so I'll extract the table first.

| **Technical Data/Specified Values** | **Details**                                      |
|-------------------------------------|--------------------------------------------------|
| **Engine Code**                     | MV                                               |
| **Type**                            | 2.1 liter 70 kW 90 SAE net HP                    |
| **Introduction**                    | October 1985                                     |
| **Part No.** (Control unit)         | 025 906 222                                      |
|-------------------------------------|--------------------------------------------------|
| **Ignition Timing Checking Spec.**  | 3-7° before TDC                                  |
| **Ignition Timing Adjusting Spec.** | 5 ± 1° before TDC                                |
| **Test and adjustment conditions**  | 1 and 9                                          |
|-------------------------------------|--------------------------------------------------|
| **Idle Adjustment idle rpm**        | 880 ± 50 rpm                                     |
| **Idle Adjustment CO content**      | 0.7 ± 0.4 Vol. %                                 |
|-------------------------------------|--------------------------------------------------|
| **Test and Adjustment Conditions**  | 1 to 6, 7, 8                                     |

With the table extracted, I can see that the idle CO content is 0.7 ± 0.4 Vol. %.

3. Error Correction
I'll double check the pages that could be relevant, but it looks like this should be the correct answer. I just double checked the values in the table,
and it looks like 0.7 ± 0.4 Vol. % is the correct value. It looks like I only used page 44 for this, so I'll just return that.

4. Final Answer
<final-answer>
The idle CO content is 0.7 ± 0.4 Vol. %.
</final-answer>
<page-references>
44
</page-references>
</example-1>
"""

example_2 = """
<example-2>
Query: Where can I find information on the Sunroof?

1. Page Scan

It looks like there are 136 pages in total, so I'll sweep through them by 10s.

Pages 1-10: These pages are the cover, title page, copyright, table of contents, and foreword. No sunroof information.

Pages 11-20: The index on pages 9-18 and continuation on 20 doesn't list "sunroof" explicitly, but I'll keep an eye out for related terms like "roof" or "top."

Pages 21-30: These pages cover general information, engine identification and some specifications. No mention of the sunroof.

Pages 31-40: These pages continue with engine removal and installation procedures. No sunroof information here.

Pages 41-50: These pages continue covering engine-related procedures. No sunroof information.

Pages 51-60: These pages deal with air-cooled engine components. No sunroof information.

Pages 61-70: Still working through the air-cooled engine section and the diesel engine section. Nothing on the sunroof.

Pages 71-80: More on engine crankshaft and crankcase, now including water-cooled engines. Still no sunroof.

Pages 81-90: Still engine-related content, but nothing about the sunroof.

Pages 91-100: These pages continue on crankshaft/crankcase information. Nothing related to the sunroof is present.

Pages 101-110: These pages cover crankshaft/crankcase information, including replacing procedures. No sunroof details.

Pages 111-120: Cylinder head and valve drive information is covered in these pages.  Still no mention of the sunroof.

Pages 121-130: More information on cylinder heads and pushrod tubes. No sunroof information.

Pages 131-140: Final pages related to cylinder heads.  No sunroof information is present.
----

Pages 4-5 Table of Contents: It contains information on the body which contains an entry for Sunroof. This entry on Sunroof covers pages 62 to 63.

Pages 62-63: No information on the sunroof.

Pages 55-64: I'll examine this range more closely since the table of contents can be inaccurate due to the non-sequential page numbering. Pages 58 and 59 have information on the sunroof, labelled as "Sunroof."


2. Extraction 

Page 4 shows "Body" has a sub-section for "Sunroof" listed as pages 62-63.

Page 58 and 59: Show the title of Sunroof.

3. Error Correction

Page 4 is the index of the manual, so it in and of itself is not relevant. Also, the index shows that the relavant pages are 62-63, but after rechecking pages 50-60, I found information on the sunroof on pages 58 and 59, titled "Sunroof."

4. Final Answer

4. Final Answer
<final-answer>
Information on the sunroof can be found on pages 58 and 59 of the manual.
</final-answer>
<page-references>
58, 59
</page-references>
"""

def get_prompt(query):
    return f"""Based on the manual pages provided, answer the following question: {query}

Please provide your response in four parts:
1. Page Scan: Explain your reasoning process, including which pages you looked at and why. Please exhaustively check every page in the input, and talk about your thoughts about each set of 10 pages. Like, I will first look at 1-10. I see nothing related to my query here. I now processed 11-20, and so on for all of the input. There are {len(pages)} pages in total, don't forget the ones on the end!
2. Extraction: For the given pages, extract the page contents. If the answer is in a table or diagram, extract the entire table / diagram, so that you can clearly see the data you want to extract.
3. Error Correction: If you made a mistake, or need to look at a different page, use this space to look at that page and extract data as needed. If no errors are detected, write "No errors detected", and list the final list of pages that you plan on returning. 
4. Final Answer: Give the precise answer to the question, as well as the pages referenced (it is possible that the answer is simply pages).

Format your response as follows:
1. Page Scan:
[your comprehensive page scan here]

2. Extraction:
[your detailed extraction here]

3. Error Correction:
[your detailed error correction here]

4. Final Answer
<final-answer>
[your precise prose answer here]
</final-answer>
<page-references>
[page numbers here, delimited by commas]
</page-references>

Here are two example outputs for your reference, please format your response accordingly:
<begin-examples>
{example_1}
{example_2}
</end-examples>
    """
```

## Adding utility Functions
Next are 3 utility functions. A utility function is code that supports the main function, but isn’t the special sauce. The first function extract_answers_from_text() extracts the final answer and page references from the model response. The second function display_selected_pages() displays the relevant pages to the user. The third function get_answer_from_manual() calls the model, then uses the 2 utility functions to give the user an answer.

That is how it works, next I’ll tell you what it works for then cover limitations.

```{python}
def extract_answer_and_references(text):
    """
    Extract the final answer and page references from the formatted text.

    Args:
        text (str): Input text containing final answer and page references in XML-like format

    Returns:
        tuple: (final_answer, page_references)
            - final_answer (str): The extracted answer text
            - page_references (list): List of page numbers as integers

    Example:
        >>> text = '''<final-answer>The idle CO content is 0.7 ± 0.4 Vol. %.</final-answer>
        ... <page-references>44, 53</page-references>'''
        >>> extract_answer_and_references(text)
        ('The idle CO content is 0.7 ± 0.4 Vol. %.', [44, 53])
    """
    # Extract final answer
    answer_match = re.search(r"<final-answer>(.*?)</final-answer>", text, re.DOTALL)
    final_answer = answer_match.group(1).strip() if answer_match else None

    # Extract page references
    ref_match = re.search(r"<page-references>(.*?)</page-references>", text, re.DOTALL)
    page_references = []

    if ref_match:
        # Split by comma and convert to integers, handling whitespace
        refs = ref_match.group(1).strip()
        page_references = [
            int(page.strip()) for page in refs.split(",") if page.strip().isdigit()
        ]

    if final_answer is None:
        raise ValueError("No final answer found in the input text")

    return final_answer, page_references

def display_selected_pages(pages, indexes, columns=2):
    """
    Displays specific pages based on their indexes, arranged in a grid with a configurable number of columns.
    Safely handles invalid indices by skipping them.

    Args:
        pages (list): List of PIL.Image objects representing the pages of a PDF.
        indexes (list): List of indices (can be any type) representing the pages to display.
        columns (int): Number of columns per row (default is 2).
    """
    # Validate columns
    if columns < 1:
        raise ValueError("The number of columns must be at least 1.")

    # Convert and filter valid indexes
    valid_indexes = []
    skipped_indexes = []

    for idx in indexes:
        try:
            # Try to convert to integer
            int_idx = int(idx)
            # Check if index is in valid range
            if 0 <= int_idx < len(pages):
                valid_indexes.append(int_idx)
            else:
                skipped_indexes.append(idx)
        except (ValueError, TypeError):
            # If conversion fails, add to skipped list
            skipped_indexes.append(idx)

    if skipped_indexes:
        print(f"Skipped invalid indexes: {skipped_indexes}")

    if not valid_indexes:
        print("No valid indexes provided. Nothing to display.")
        return

    # Calculate rows needed
    rows = math.ceil(len(valid_indexes) / columns)

    # Create a grid to display pages
    fig, axes = plt.subplots(rows, columns, figsize=(columns * 5, rows * 7))
    # Convert axes to 2D array if it's 1D or a single axis
    if rows == 1 and columns == 1:
        axes = np.array([[axes]])
    elif rows == 1 or columns == 1:
        axes = axes.reshape(-1, columns)
    axes = axes.flatten()  # Flatten for easier indexing

    # Iterate over valid indexes and plot
    for i, index in enumerate(valid_indexes):
        axes[i].imshow(pages[index - 1])  # Render the page in color
        axes[i].axis("off")  # Remove axes for cleaner display
        axes[i].set_title(f"Page Number: {index}")  # Set title as page index

    # Hide unused subplots
    for j in range(len(valid_indexes), len(axes)):
        axes[j].axis("off")

    plt.tight_layout()
    plt.show()

indexes_to_display = [1, 2, "A", -1, 5, 8]  # Example with some invalid indices
display_selected_pages(pages, indexes_to_display, columns=3)

def get_answer_from_manual(query, model=model, pages=pages):
    """
    Query the model about the manual and return the answer with relevant pages.

    Args:
        model: The generative AI model instance
        pages: List of PDF pages
        query (str): The question to ask about the manual

    Returns:
        tuple: (answer, page_numbers, raw_response)
            - answer (str): The extracted final answer
            - page_numbers (list): List of relevant page numbers
            - raw_response (str): The complete raw response from the model

    Raises:
        ValueError: If no final answer is found in the response
    """
    # Format and send the prompt
    prompt = get_prompt(query=query)
    response = model.generate_content(contents=[prompt])
    response_text = response.candidates[0].content.parts[0].text

    # Extract answer and page numbers
    answer, page_nums = extract_answer_and_references(response_text)

    print(answer)
    display_selected_pages(pages, page_nums)

    return answer, page_nums, response_text
```

## Using the Tool
Below are examples of the tool in use.

What I did here is important, you actually decide what queries you want it to handle then use that to assess how well it works.
Anything a technician wants to know has a location and a format. 
Information in the manual has a location and a format. Sometimes a technician just wants the location, or just the information, or a specific format. So we made the tool to be able to handle these types of queries. Because the manual we were working off of had a lot of information in textual procedurals, and diagrams of exploded schematics, we 

Query 1: Looking for location
In the example below, 


We thought through what kinds of queries a technician would have and tested the 4 most frequent query types:

The technician wants to know where information is in the manual
The technician is looking for a procedure
The technician is looking for a diagram
The technician is looking for a detail
Information in machine manuals is often multimodal, containing text and images. In some cases a technician doesn't care what mode the information comes in (query types 1 and 4), in others they are looking for a specific mode (query types 2 and 3).

In this demonstration's manual, the modes are procedures, data tables, and a host of visuals like exploded schematics, circuit diagrams, and troubleshooting flow charts. Since the majority of the manual's content is procedures and exploded schematics, we focused on those examples

```{=html}
<iframe src="https://www.kaggle.com/embed/mhewlett/google-gemini-long-context?cellIds=31&kernelSessionId=210373422" height="500" style="margin: 0 auto; width: 100%; max-width: 950px;" frameborder="0" scrolling="auto" title="Google - Gemini Long Context"></iframe>
```
```{=html}
<iframe src="https://www.kaggle.com/embed/mhewlett/google-gemini-long-context?cellIds=32&kernelSessionId=210373422" height="500" style="margin: 0 auto; width: 100%; max-width: 950px;" frameborder="0" scrolling="auto" title="Google - Gemini Long Context"></iframe>
```
```{=html}
<iframe src="https://www.kaggle.com/embed/mhewlett/google-gemini-long-context?cellIds=33&kernelSessionId=210373422" height="500" style="margin: 0 auto; width: 100%; max-width: 950px;" frameborder="0" scrolling="auto" title="Google - Gemini Long Context"></iframe>
```
```{=html}
<iframe src="https://www.kaggle.com/embed/mhewlett/google-gemini-long-context?cellIds=34&kernelSessionId=210373422" height="500" style="margin: 0 auto; width: 100%; max-width: 950px;" frameborder="0" scrolling="auto" title="Google - Gemini Long Context"></iframe>
```

## Performance and Limitations

The point of the tool is find information for technicians faster. The QA tool does well on finding the right information. The technicians I talked to say that they either have a physical manul, or use ctrl+F in a PDF to search for keywords. This tool works better than keyword search because it also lets technicians search by images or diagrams that have no associated text, that could not have been found with the normal method. On the other side, the tool takes about 1 minute to generate a response so it fails to find the information for technicians faster. One solution I am working on is RAG since there would be less information for the model to process in generating an answer, so response time should be faster. The second limitation is reliability. We have examples, but don’t know how often it is right or wrong. To address this we’ll make a bunch of example queries and correct responses in the different query types, then run the tests multiple times to score the precision and recall of the model. This will let us talk about how good the tool isand when we make changes, to what extent is has improved.

## Conclusion
May need to write